{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimization\n",
    "This notebook goes through the whole process to use the optimized model and produce the synthesis planning."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment Setup\n",
    "First of all, we need to make sure that the notebook is running in the correct environment.\n",
    "\n",
    "To do that, follow these steps:\n",
    " 1. Create the project's environment\n",
    "    To do that, place yourself in the project's root, and run :\n",
    "    `conda env create -f environment.yml`\n",
    "    This creates a new clean conda environment with the package needed by the project.\n",
    " 2. Activate the environment\n",
    "    On Linux and Mac:\n",
    "    `source activate synnet`\n",
    "    On Windows:\n",
    "    `conda activate synnet`\n",
    " 3. Install the project's module\n",
    "    Now that the environment is activated, we need to install the project as a module.\n",
    "    Place yourself in the project's root and run :\n",
    "    `pip install -e .`\n",
    " 4. Restart Jupyter from the new environment\n",
    "    Now, we can start Jupyter from the environment, that way it has all the dependencies we need. Simply run `jupyter notebook` and open this notebook.\n",
    "\n",
    "To test the setup, run the following cell."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from reproducibility.helpers import paths\n",
    "\n",
    "# Check that the correct conda env is being used\n",
    "if sys.prefix.split(\"\\\\\")[-1] != \"synnet\":\n",
    "    print(\n",
    "        \"You are not using the correct conda environment, please follow the instructions above\"\n",
    "    )\n",
    "else:\n",
    "    try:\n",
    "        import synnet\n",
    "\n",
    "        print(\"The environment is setup correctly\")\n",
    "    except ImportError:\n",
    "        print(\n",
    "            \"The module 'synnet' is not installed, please follow the instructions above\"\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-Processing\n",
    "\n",
    "Now that the conda environment is correctly setup, we can start the preliminary steps to produce the synthesis results.\n",
    "\n",
    "First, let's import some packages, define some constants.\n",
    "Make sure they are correct."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from helpers.loader import *\n",
    "from helpers.preprocessor import *\n",
    "from helpers.optimize import optimize\n",
    "\n",
    "# Number of cores to use for the computation. The greater, the faster\n",
    "cpu_cores = 6\n",
    "# Number of molecules to randomly pick from the datasets.\n",
    "# Our results were made with a sample of 10000\n",
    "num_samples = 10000\n",
    "# Seed to use to sample the datasets\n",
    "seed = 42"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data\n",
    "First, we need to choose the trained model to use.\n",
    "\n",
    "For now, we only have the one provided by the paper's authors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "original_checkpoints = get_original_checkpoints()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And the model we trained"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trained_checkpoints = get_trained_checkpoints()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we need to retrieve the building blocks. We asked the company to provide them, that way we can correctly reproduce their result\n",
    "\n",
    "To simplify the workflow, this also perform the step 0 described in INSTRUCTIONS.md"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bblocks_raw = get_building_blocks()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also need to download the molecules we want to test the model on.\n",
    "\n",
    "We will use three datasets :\n",
    " - the ZINC dataset\n",
    "  - the ChEMBL dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "zinc_smiles = get_zinc_dataset(num_samples, seed)\n",
    "chembl_smiles = get_chembl_dataset(num_samples, seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Process Building Blocks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filter Building Blocks\n",
    "First, we apply the step 1 from INSTRUCTION.md\n",
    "\n",
    "We pre-process the building blocks to identify applicable reactants for each reaction template. In other words, filter out all building blocks that do not match any reaction template. There is no need to keep them, as they cannot act as reactant.\n",
    "\n",
    "In a first step, we match all building blocks with each reaction template.\n",
    "In a second step, we save all matched building blocks and a collection of `Reactions` with their available building blocks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bblocks, rxn_collection = filter_bblocks(bblocks_raw)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-compute embeddings\n",
    "Then, step 2\n",
    "\n",
    "We use the embedding space for the building blocks a lot. Hence, we pre-compute and store the building blocks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mol_embedder = compute_embeddings(bblocks, cpu_cores)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimize\n",
    "\n",
    "Now that everything is loaded and pre-processed, we can do the optimization.\n",
    "\n",
    "First, on the ZINC dataset, with the original model and the one we trained."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimize(\n",
    "    zinc_smiles,\n",
    "    bblocks,\n",
    "    rxn_collection,\n",
    "    original_checkpoints,\n",
    "    mol_embedder,\n",
    "    paths.optimize_result_path(\"zinc\", \"original\"),\n",
    "    nbits=4096,\n",
    "    num_gen=2,\n",
    "    objective=\"gsk\",\n",
    "    rxn_template=\"hb\",\n",
    "    num_offspring=128,\n",
    "    cpu_cores=cpu_cores,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimize(\n",
    "    zinc_smiles,\n",
    "    bblocks,\n",
    "    rxn_collection,\n",
    "    trained_checkpoints,\n",
    "    mol_embedder,\n",
    "    paths.optimize_result_path(\"zinc\", \"trained\"),\n",
    "    nbits=4096,\n",
    "    num_gen=2,\n",
    "    objective=\"gsk\",\n",
    "    rxn_template=\"hb\",\n",
    "    num_offspring=128,\n",
    "    cpu_cores=cpu_cores,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, on the ChEMBL dataset, also with the original model and the one we trained."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimize(\n",
    "    chembl_smiles,\n",
    "    bblocks,\n",
    "    rxn_collection,\n",
    "    original_checkpoints,\n",
    "    mol_embedder,\n",
    "    paths.optimize_result_path(\"chembl\", \"original\"),\n",
    "    nbits=4096,\n",
    "    num_gen=2,\n",
    "    objective=\"gsk\",\n",
    "    rxn_template=\"hb\",\n",
    "    num_offspring=128,\n",
    "    cpu_cores=cpu_cores,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimize(\n",
    "    chembl_smiles,\n",
    "    bblocks,\n",
    "    rxn_collection,\n",
    "    original_checkpoints,\n",
    "    mol_embedder,\n",
    "    paths.optimize_result_path(\"chembl\", \"trained\"),\n",
    "    nbits=4096,\n",
    "    num_gen=2,\n",
    "    objective=\"gsk\",\n",
    "    rxn_template=\"hb\",\n",
    "    num_offspring=128,\n",
    "    cpu_cores=cpu_cores,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
