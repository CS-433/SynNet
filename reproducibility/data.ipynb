{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook analyzes the data used for training the models. Data generation and training have been done following and running all the steps described in [INSTRUCTIONS.MD](https://github.com/wenhao-gao/SynNet/blob/master/INSTRUCTIONS.md) on the original repository (using the scripts provided in the 'scripts' folder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Environment Setup\n",
    "First of all, we need to make sure that the notebook is running in the correct environment.\n",
    "\n",
    "To do that, follow these steps:\n",
    " 1. Create the project's environment\n",
    "    To do that, place yourself in the project's root, and run :\n",
    "    `conda env create -f environment.yml`\n",
    "    This creates a new clean conda environment with the package needed by the project.\n",
    " 2. Activate the environment\n",
    "    On Linux and Mac:\n",
    "    `source activate synnet`\n",
    "    On Windows:\n",
    "    `conda activate synnet`\n",
    " 3. Install the project's module\n",
    "    Now that the environment is activated, we need to install the project as a module.\n",
    "    Place yourself in the project's root and run :\n",
    "    `pip install -e .`\n",
    " 4. Restart Jupyter from the new environment\n",
    "    Now, we can start Jupyter from the environment, that way it has all the dependencies we need. Simply run `jupyter notebook` and open this notebook.\n",
    "\n",
    "To test the setup, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Check that the correct conda env is being used\n",
    "if sys.prefix.split(\"\\\\\")[-1] != \"synnet\":\n",
    "    print(\n",
    "        \"You are not using the correct conda environment, please follow the instructions above\"\n",
    "    )\n",
    "else:\n",
    "    try:\n",
    "        import synnet\n",
    "\n",
    "        print(\"The environment is setup correctly\")\n",
    "    except ImportError:\n",
    "        print(\n",
    "            \"The module 'synnet' is not installed, please follow the instructions above\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree generation and splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic trees describe the chemical synthesis of one molecule. These data are key because they are fed to the model, determining its accuracy. First, we download the filtered synthetic trees sets. These files contain around 450000 synthetic trees that have been generated using scripts 03 (03-generate-syntrees.py), filtered using script 04 (04-filter-trees.py) to obtain trees containing drug-like molecule roots, and splitted into train, validation and test sets using script 05 (05-split-syntrees.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Descriptors import MolWt\n",
    "from helpers.loader import get_filtered_syntrees, get_featurized_syntrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load each set and check number of trees contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees_dict = get_filtered_syntrees()\n",
    "\n",
    "train = trees_dict[\"train\"]\n",
    "test = trees_dict[\"test\"]\n",
    "valid = trees_dict['valid']\n",
    "syntrees = trees_dict.items()\n",
    "\n",
    "print(\"Loaded content :\\n\")\n",
    "\n",
    "for name, trees in syntrees:\n",
    "    print(f\"{name} set contains {len(trees)} trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A synthetic tree is formed by a dictionary containing different keys: reactions, chemicals, depth, actions and rxn_id2type. By analyzing feature distribution on each tree set, we can check if all sets have the same data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check one tree structure\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check tree depth distribution for each set. Tree depth determines the complexity of the synthesis (the more steps a synthesis has, the more challenging it is). Therefore, we would like to know how the different synthesis are distributed and their complexity distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tree depth frequency for each set\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "for i, (name, trees) in enumerate(syntrees):\n",
    "    depth = [int(tree[\"depth\"]) for tree in trees]\n",
    "    depth = pd.Series(depth)\n",
    "    counts = depth.value_counts(normalize=True)\n",
    "    ind = counts.index\n",
    "    freq = counts.values\n",
    "\n",
    "    ax[i].bar(ind, freq)\n",
    "    ax[i].set_xlim((0, 11))\n",
    "    ax[i].set_xticks(list(range(0, 11)))\n",
    "    ax[i].set_title(name)\n",
    "\n",
    "fig.supylabel(\"frequency\")\n",
    "fig.supxlabel(\"depth\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these figures we can observe that the distribution is similar in the three sets. There is a predominance of depth = 1 trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also analyze how actions are distributed in all trees. Actions are also related to the synthetic pathways (for example, higher number of expand actions would mean more challenging synthesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tree action frequency for each set\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "for i, (name, trees) in enumerate(syntrees):\n",
    "    actions = [tree[\"actions\"] for tree in trees]\n",
    "    actions = [act for group in actions for act in group]\n",
    "    actions = pd.Series(actions)\n",
    "    counts = actions.value_counts(normalize=True)\n",
    "    ind = counts.index\n",
    "    freq = counts.values\n",
    "    ax[i].bar(ind, freq)\n",
    "    ax[i].set_xticks(ticks=[0, 1, 2, 3], labels=[\"add\", \"expand\", \"merge\", \"end\"])\n",
    "    ax[i].set_title(name)\n",
    "\n",
    "fig.supylabel(\"Frequency\")\n",
    "fig.supxlabel(\"Action\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again observe the same distribution in all sets. Actions 1 and 2 have a joint frequency lower than 0.15, meaning that the majority of trees won't be branched "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we check root products (stored as SMILES in each tree). We use molar weight as an approximate measure of product complexity distribution, checking molar weight distribution for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tree action frequency for each set\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "for i, (name, trees) in enumerate(syntrees):\n",
    "    smiles = [tree[\"root\"][\"smiles\"] for tree in trees]\n",
    "    mw = [MolWt(Chem.MolFromSmiles(smile)) for smile in smiles]\n",
    "\n",
    "    ax[i].hist(mw, bins=50)\n",
    "    ax[i].set_xlim(0, 1500)\n",
    "    ax[i].set_title(name)\n",
    "\n",
    "fig.supylabel(\"N molecules\")\n",
    "fig.supxlabel(\"Molar weight\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Molar weights are similarly distributed on the three sets, showing that the majority of root molecules have a value between 250 and 500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous graphs show that synthetic tree data are similar for the three sets, and therefore we can conclude that data (root molecules, synthetic steps and applied reactions) will have the same distribution in the three cases. We thus expect that the model will build root molecules from the test set (reachable) easier than molecules coming from a different data distribution (like ChEMBL dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tree generation, we featurize them using scripts 06 (06-featurize-syntrees.py). Featurization splits each tree in the corresponding synthetic steps and creates a vector embedding representing the state of the tree (this data can be fed to each MLP to train the model). Besides, we split the features into the corresponding actions to fed each type of network using script 07 (07-split-data-for-networks.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Featurized data can be downloaded by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_featurized_syntrees()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data are used to train each MLP from scratch by running the corresponding scripts stored on src/synnet/models/. The resulting trained network are used in run.ipynb for synthesis planning and result evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('synnet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c2aacb62e6b2d465adc9f7702b10567223f45b98f5bfcc8a0f9c28f90eb2f2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
