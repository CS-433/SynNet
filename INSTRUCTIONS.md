# Instructions

This documents outlines the process to train SynNet from scratch step-by-step.

> :warning: It is still a WIP to match the filenames of the scripts to the instructions here and to simplify the dependency on parameters/filenames.

You can use any set of reaction templates and building blocks, but we will illustrate the process with the *Hartenfeller-Button* reaction templates and *Enamine building blocks*.

*Note*: This project depends on a lot of exact filenames.
For example, one script will save to file, the next will read that file for further processing.
It is not a perfect approach - we are open to feedback - and advise to revise the parameters defined in each script.

Let's start.

## Step-by-Step

0. Prepare reaction templates and building blocks.

    Extract SMILES from the `.sdf` file from enamine.net.

    ```shell
    python scripts/00-extract-smiles-from-sdf.py --file="data/assets/building-blocks/enamine-us.sdf"
    ```

1. Filter building blocks.

    We proprocess the building blocks to identify applicable reactants for each reaction template.
    In other words, filter out all building blocks that do not match any reaction template.
    There is no need to keep them, as they cannot act as reactant.
    In a first step, we match all building blocks with each reaction template.
    In a second step, we save all matched building blocks.

    ```bash
    # Match
    python scripts/01-filter-building-blocks.py \
        --building-blocks-file "data/assets/building-blocks/enamine-us-smiles.csv.gz"  \
        --rxn-templates-file "data/assets/reaction-templates/hb.txt"  \
        --output-file "data/pre-process/building-blocks/enamine-us-smiles.csv.gz" \
        --verbose
    ```

    > :bulb: All following steps use this matched building blocks <-> reaction template data. You have to specify the correct files for every script to that it can load the right data. It can save some time to store these as environment variables.

2. Pre-compute embeddings

    We use the embedding space for the building blocks a lot.
    Hence, we pre-compute and store the building blocks.

    ```bash
    python scripts/02-compute-embeddings.py \
        --building-blocks-file "data/pre-process/building-blocks/enamine-us-smiles.csv.gz" \
        --rxn-templates-file "data/assets/reaction-templates/hb.txt"
        --output-file "data/pre-process/embeddings/hb-enamine-embeddings.npy"
    ```

3. Generate *synthetic trees*

    Herein we generate the data used for training the networks.
    The data is generated by randomly selecting building blocks, reaction templates and directives to grow a synthetic tree.

    ```bash
    # Generate synthetic trees
    python scripts/03-generate-syntrees.py \
        --building-blocks-file "data/pre-process/building-blocks/enamine-us-smiles.csv.gz" \
        --rxn-templates-file   "data/assets/reaction-templates/hb.txt" \
        --output-file          "data/pre-process/synthetic-trees.json.gz" \
        --number-syntrees 600000
    ```

    In a second step, we filter out some synthetic trees to make the data pharmaceutically more interesting.
    That is, we filter out trees, whose root node molecule has a QED < 0.5, or randomly with a probability less than 1 - QED/0.5.

    ```bash
    # Filter
    python scripts/04-filter-synthetic-trees.py \
        --input-file  "data/pre-process/synthetic-trees.json.gz" \
        --output-file "data/pre-process/synthetic-trees-filtered.json.gz"
    ```

    Each *synthetic tree* is serializable and so we save all trees in a compressed `.json` file.

5. Split *synthetic trees* into train,valid,test-data

    We load the `.json`-file with all *synthetic trees* and
    straightforward split it into three files: `{train,test,valid}.json`.
    The default split ratio is 6:2:2.

    ```bash
    python scripts/05-split-syntrees.py \
        --input-file "data/pre-process/synthetic-trees-filtered.json.gz"
        --output-dir "data/pre-process/split"
    ```

6. Featurization

   > :bulb: All following steps depend on the representations for the data. Hence, you have to specify the parameters for the representations as input argument for most of the scripts so that it can operate on the right data.

   We featurize each *synthetic tree*.
   That is, we break down each tree to each iteration step ("Add", "Expand", "Extend", "End") and featurize it.
   This results in a "state" vector and a a corresponding "super step" vector.
   We call it "super step" here, as it contains all (featurized) data for all networks.

    ```bash
    python scripts/06-featurize-syntrees.py \
        --input-file "data/pre-process/split/synthetic-trees-train.json.gz" # or {train,valid,test}
        --output-dir "data/featurized"
    ```

    This script will load the `input-file`, featurize it, and it in
      - `<output-dir>/hb_fp_2_4096_fp_256/states_{train,valid,test}.np` and
      - `<output-dir>/hb_fp_2_4096_fp_256/steps_{train,valid,test}.np`.

7. Split features

    Up to this point, we worked with a (featurized) *synthetic tree* as a whole,
    now we split it up to into "consumable" input/output data for each of the four networks.
    This includes picking the right featurized data from the "super step" vector from the previous step.

    ```bash
    python scripts/08-split-data-for-networks.py \
        --input-dir "data/featurized/hb_fp_2_4096_fp_256"
    ```

8. Train the networks

    Finally, we can train each of the four networks in `src/syn_net/models/` separately:

    ```bash
    python src/syn_net/models/act.py
    ```

After training a new model, you can then use the trained model to make predictions and construct synthetic trees for a list given set of molecules.

You can also perform molecular optimization using a genetic algorithm.

Please refer to the [README.md](./README.md) for inference instructions.

## Auxiallary Scripts

### Visualizing trees

To be added.

### Mean reciprocal rank

To be added.
